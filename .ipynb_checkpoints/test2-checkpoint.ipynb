{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SigmoidActivationFunction:\n",
    "    \n",
    "    @staticmethod\n",
    "    def value(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        sig = SigmoidActivationFunction.value(z)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "class WeightPacking:\n",
    "    @staticmethod\n",
    "    def pack(weights, biases):\n",
    "        return np.concatenate([np.concatenate([w.ravel(), b.ravel()]) for w,b in zip(weights, biases)])\n",
    "    \n",
    "    @staticmethod\n",
    "    def unpack(thetas, layers):\n",
    "        start = 0\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for x in range(len(layers)-1):\n",
    "            y,x = layers[x], layers[x+1]\n",
    "            weights.append(thetas[start:start+y*x].reshape(y,x))\n",
    "            start += y*x\n",
    "            biases.append(thetas[start:start+x].reshape(1, x))\n",
    "            start += x\n",
    "        return (weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NN_1HL(object):\n",
    "    \n",
    "    def __init__(self, reg_lambda=0, epsilon_init=0.12,\n",
    "                 hidden_layer_size=25, opti_method='TNC', maxiter=500, ActivationFunction = SigmoidActivationFunction):\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.epsilon_init = epsilon_init\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.activation_func = ActivationFunction.value\n",
    "        self.activation_func_prime = ActivationFunction.derivative\n",
    "        self.method = opti_method\n",
    "        self.maxiter = maxiter\n",
    "    \n",
    "    def rand_init(self, l_in, l_out):\n",
    "        return np.random.rand(l_out, l_in + 1) * 2 * self.epsilon_init - self.epsilon_init\n",
    "    \n",
    "    def pack_thetas(self, t1, t2):\n",
    "        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))\n",
    "    \n",
    "    def unpack_thetas(self, thetas, layers):\n",
    "        \n",
    "        input_layer_size, hidden_layer_size, num_labels = layers[0], layers[1], layers[2]\n",
    "        \n",
    "        t1_start = 0\n",
    "        t1_end = hidden_layer_size * (input_layer_size + 1)\n",
    "        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))\n",
    "        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))\n",
    "        return t1, t2\n",
    "    \n",
    "\n",
    "    def costCalc(self, actual, predicted):\n",
    "        costPositive = -actual * np.log(predicted)\n",
    "        costNegative = (1 - actual) * np.log(1 - predicted)\n",
    "        cost = costPositive - costNegative\n",
    "        return np.sum(cost)\n",
    "    \n",
    "    def function(self, thetas, layers, X, y, reg_lambda):\n",
    "        \n",
    "        t1, t2 = self.unpack_thetas(thetas, layers)\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        Y = np.eye(layers[-1])[y]\n",
    "        \n",
    "        _, _, _, _, h = self._forward(X, t1, t2)\n",
    "\n",
    "        J = self.costCalc(Y, h) / m\n",
    "        \n",
    "        if reg_lambda != 0:\n",
    "            t1f = t1[:, 1:]\n",
    "            t2f = t2[:, 1:]\n",
    "            J += self.reg_lambda / (2 * m) * (np.sum(t1f**2) + np.sum(t2f**2))\n",
    "        return J\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        num_features = X.shape[0]\n",
    "        input_layer_size = X.shape[1]\n",
    "        num_labels = len(set(y))\n",
    "        \n",
    "        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)\n",
    "        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)\n",
    "        \n",
    "        \n",
    "        t1w = theta1_0[:, 1:].T\n",
    "        t1b = theta1_0[:, 0][np.newaxis]\n",
    "\n",
    "        t2w = theta2_0[:, 1:].T\n",
    "        t2b = theta2_0[:, 0][np.newaxis]\n",
    "        \n",
    "        thetas0 = WeightPacking.pack([t1w, t2w], [t1b, t2b])\n",
    "        #thetas0 = self.pack_thetas(theta1_0, theta2_0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        layers = [input_layer_size, self.hidden_layer_size, num_labels]\n",
    "        \n",
    "        options = {'maxiter': self.maxiter}\n",
    "        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, \n",
    "                                 args=(layers, X, y, self.reg_lambda), options=options)\n",
    "        \n",
    "        self.t1, self.t2 = self.unpack_thetas(_res.x, [input_layer_size, self.hidden_layer_size, num_labels])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).argmax(0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        _, _, _, _, h = self._forward(X, self.t1, self.t2)\n",
    "        return h.T\n",
    "    \n",
    "    def _forward(self, X, t1, t2):\n",
    "        t1w = t1[:, 1:].T\n",
    "        t1b = t1[:, 0][np.newaxis]\n",
    "\n",
    "        t2w = t2[:, 1:].T\n",
    "        t2b = t2[:, 0][np.newaxis]\n",
    "        \n",
    "        \n",
    "        a1new = X\n",
    "        z1new = np.dot(X, t1w) + t1b\n",
    "        a2new = self.activation_func(z1new)\n",
    "        z2new = np.dot(a2new, t2w) + t2b\n",
    "        a3new = self.activation_func(z2new)\n",
    "        \n",
    "        yd, xd = z1new.T.shape\n",
    "        if xd == 1:\n",
    "            z1new =  z1new.ravel()\n",
    "            z2new =  z2new.ravel()\n",
    "        \n",
    "        return a1new, z1new, a2new, z2new, a3new\n",
    "    \n",
    "    \n",
    "    \n",
    "    def function_prime(self, thetas, layers, X, y, reg_lambda):\n",
    "\n",
    "        t1, t2 = self.unpack_thetas(thetas, layers)\n",
    "        m = X.shape[0]\n",
    "\n",
    "        t1w = t1[:, 1:].T\n",
    "        t1b = t1[:, 0][np.newaxis]\n",
    "\n",
    "        t2w = t2[:, 1:].T\n",
    "        t2b = t2[:, 0][np.newaxis]\n",
    "        \n",
    "        \n",
    "        Y = np.eye(layers[-1])[y]\n",
    "\n",
    "        b1 = np.zeros(layers[1])\n",
    "        b2 = np.zeros(layers[2])\n",
    "        \n",
    "        Delta1, Delta2 = np.zeros(t1w.shape), np.zeros(t2w.shape)\n",
    "\n",
    "        for i, row in enumerate(X):\n",
    "            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)\n",
    "\n",
    "            # Backprop\n",
    "            d3 = (a3 - Y[i, :]).T\n",
    "            d2 = np.dot(t2w, d3).ravel() * self.activation_func_prime(z2)\n",
    "            \n",
    "            b2+=d3.ravel()\n",
    "            b1+=d2.ravel()\n",
    "            \n",
    "            Delta2 += np.dot(d3, a2).T\n",
    "            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis]).T\n",
    "            \n",
    "            \n",
    "        Theta1_grad = np.zeros(t1.shape)\n",
    "        Theta1_grad[:, 1:] = Delta1.T/m\n",
    "        \n",
    "        Theta2_grad = np.zeros(t2.shape)\n",
    "        Theta2_grad[:, 1:] = Delta2.T/m\n",
    "        \n",
    "        \n",
    "        Theta1_grad[:, 0] = b1/m\n",
    "        Theta2_grad[:, 0] = b2/m\n",
    "        \n",
    "        \n",
    "        if reg_lambda != 0:\n",
    "            Theta1_grad[:, 1:] += (reg_lambda / m) * t1w.T\n",
    "            Theta2_grad[:, 1:] += (reg_lambda / m) * t2w.T\n",
    "        \n",
    "        return self.pack_thetas(Theta1_grad, Theta2_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96666666666666667"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(40)\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn import cross_validation\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "nn = NN_1HL(reg_lambda = 2.1)\n",
    "nn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, nn.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.96666666666666667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.876"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(40)\n",
    "from scipy.io import loadmat\n",
    "data = loadmat('ex3data1.mat')\n",
    "X, y = data['X'], data['y']\n",
    "y = y.reshape(X.shape[0], )\n",
    "y = y - 1  # Fix notation # TODO: Automaticlly fix that on the class\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "nn = NN_1HL(maxiter=50, reg_lambda = 2.1)\n",
    "nn.fit(X_train, y_train)\n",
    "\n",
    "accuracy_score(y_test, nn.predict(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.876"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#class NeuralNet:\n",
    "#    def __init__(self, reg_lambda=0, epsilon_init=0.12, hiddenLayerSizes = [25],  opti_method='TNC', maxiter=500):\n",
    "#        self.reg_lambda = reg_lambda\n",
    "#        self.epsilon_init = epsilon_init\n",
    "#        self.method = opti_method\n",
    "#        self.maxiter = maxiter\n",
    "#        self._hiddenLayerSizes = hiddenLayerSizes\n",
    "#        \n",
    "#    def weightsInit(self, layers):\n",
    "#        weights = []\n",
    "#        biases = []\n",
    "#        for x in range(len(layers)-1):\n",
    "#            weights.append(np.random.randn(layers[x], layers[x+1]))\n",
    "#            biases.append(np.random.randn(1, layers[x+1]))\n",
    "#        return weights, biases\n",
    "#        \n",
    "#    def fit(self, X, y):\n",
    "#        #print(\"yxdfs\", X.shape)\n",
    "#        numFeatures = X.shape[0]\n",
    "#        inputLayerSize = X.shape[1]\n",
    "#        numLabels = len(set(y))\n",
    "#        \n",
    "#        layers = [inputLayerSize] + self._hiddenLayerSizes + [numLabels]\n",
    "#        weights, biases = self.weightsInit(layers)\n",
    "#        \n",
    "#        \n",
    "#        #for w, b in zip(weights, biases):\n",
    "#        #    print(w.shape, b.shape)\n",
    "#        \n",
    "#        p = self.pack(weights, biases)\n",
    "#        #wt, bt = self.unpack(p, layers)\n",
    "#        #\n",
    "#        #for w, b in zip(wt, bt):\n",
    "#        #    print(w.shape, b.shape)\n",
    "#        \n",
    "#        options = {'maxiter': 500}\n",
    "#        _res = optimize.minimize(self.optimFunction, p, jac=True, method='TNC', \n",
    "#                                 args=(layers, X, y, 0), options=options)\n",
    "#        \n",
    "#        \n",
    "#        \n",
    "#        \n",
    "#        self.optimFunction(_res.x, layers, X, y, 0)\n",
    "#        \n",
    "#        self._weights, self._biases = weights, biases\n",
    "#        \n",
    "\n",
    "#            \n",
    "#        \n",
    "\n",
    "#\n",
    "#\n",
    "#        \n",
    "#    def optimFunction(self, thetas, layers,  X, y, reg_lambda):\n",
    "#        weights, biases = self.unpack(thetas, layers)\n",
    "#        \n",
    "#        m = X.shape[0]\n",
    "#        wt, bt = self.unpack(thetas, layers)\n",
    "#        Y = np.eye(layers[-1])[y]\n",
    "#        \n",
    "#\n",
    "#        predicted = self._forward(X, weights, biases)\n",
    "#        \n",
    "#        #J = np.sum(np.nan_to_num(-Y * np.log(predicted) - (1 - Y) * np.log(1 - predicted))) / m\n",
    "#        #print(J)\n",
    "#        \n",
    "#        nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "#        nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "#        \n",
    "#        for i, row in enumerate(X):\n",
    "#            \n",
    "#            \n",
    "#            #Forward\n",
    "#            inputActivations = [X[i,:]]\n",
    "#            outputActivations = []\n",
    "#            for w,b in zip(weights, biases):\n",
    "#                z = np.dot(inputActivations[-1], w)+b\n",
    "#                outputActivations.append(z)\n",
    "#                inputActivations.append(self.sigmoid(z))\n",
    "#            predicted = inputActivations[-1]\n",
    "#            \n",
    "#            #Back\n",
    "#            ds = [predicted - Y[i, :]]\n",
    "#            for x in range(len(weights)-1):\n",
    "#                ds.append(np.dot(ds[-1], weights[1].T) * self.sigmoid_prime(outputActivations[0]))\n",
    "#               \n",
    "#            \n",
    "#            \n",
    "#            nabla_b = [d+b for b, d in zip(reversed(ds), nabla_b)]\n",
    "#            \n",
    "#            #print(inputActivations[0].shape)\n",
    "#            \n",
    "#            Delta2 = np.dot(inputActivations[1].T, ds[0])\n",
    "#            Delta1 = np.dot(inputActivations[0][np.newaxis].T, ds[1])\n",
    "#            \n",
    "#            nabla_w = [nabla_w[0] + Delta1, nabla_w[1] + Delta2]\n",
    "#            \n",
    "#            \n",
    "#        Thetagrad = [w/m for w in nabla_w]\n",
    "#        Thetagradbias = [b/m for b in  nabla_b]\n",
    "#        \n",
    "#        return (J, self.pack(Thetagrad, Thetagradbias))\n",
    "#            \n",
    "#        \n",
    "#        \n",
    "#        \n",
    "#        \n",
    "#    def sigmoid(self, X):\n",
    "#        return 1 / (1 + np.exp(-X))\n",
    "#    \n",
    "#    def sigmoid_prime(self, z):\n",
    "#        sig = self.sigmoid(z)\n",
    "#        return sig * (1 - sig)\n",
    "#        \n",
    "#    def _forward(self, X, weights, biases):        \n",
    "#        for w, b in zip(weights, biases):\n",
    "#            X = self.sigmoid(np.dot(X, w) + b)\n",
    "#        return X\n",
    "#    \n",
    "#    def predict(self, X):\n",
    "#        return self._forward(X, self._weights, self._biases).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.random.seed(40)\n",
    "#import sklearn.datasets as datasets\n",
    "#from sklearn import cross_validation\n",
    "#\n",
    "#iris = datasets.load_iris()\n",
    "#X = iris.data\n",
    "#y = iris.target\n",
    "#\n",
    "#X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4)\n",
    "#\n",
    "#nn = NeuralNet(reg_lambda = 2.1)\n",
    "#nn.fit(X_train, y_train)\n",
    "#\n",
    "#\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#\n",
    "#accuracy_score(y_test, nn.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.82353265,  0.10169697,  0.37523681],\n",
       "       [ 0.64959551,  0.99375116,  0.39973218],\n",
       "       [ 0.31309426,  0.84499566,  0.61286451],\n",
       "       [ 0.85020265,  0.69461428,  0.16775967],\n",
       "       [ 0.33532488,  0.8425862 ,  0.10078412],\n",
       "       [ 0.69056047,  0.48535535,  0.76482521],\n",
       "       [ 0.15156521,  0.39489849,  0.40561274],\n",
       "       [ 0.00675989,  0.69183383,  0.32111048],\n",
       "       [ 0.84364198,  0.25563488,  0.76862291],\n",
       "       [ 0.13310176,  0.77821092,  0.31205836]])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
