{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SigmoidActivationFunction:\n",
    "    \n",
    "    @staticmethod\n",
    "    def value(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        sig = SigmoidActivationFunction.value(z)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "    \n",
    "class CrossEntropyCostFunction:\n",
    "\n",
    "    @staticmethod\n",
    "    def cost(actual, predicted, numberOfExamples):\n",
    "        return np.sum(-actual * np.log(predicted).T - (1 - actual) * np.log(1 - predicted).T) / numberOfExamples    \n",
    "    \n",
    "    @staticmethod\n",
    "    def regulazation(weightsList, lambdaFactor, numberOfelements):\n",
    "        cost = 0\n",
    "        if lambdaFactor == 0:\n",
    "            for w in weightsList:\n",
    "                cost +=np.dot(w, w)\n",
    "            return (lambdaFactor/(2*numberOfelements))*cost\n",
    "        return cost\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(actual, predicted, activationFunction):\n",
    "        return predicted-actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NN_1HL:\n",
    "    #opti_method='TNC' BFGS\n",
    "    def __init__(self, layerSizes = [10,5,3], reg_lambda=0, epsilon_init=0.12, \n",
    "                 hidden_layer_size1=25,hidden_layer_size2 = 25,  opti_method='TNC', maxiter=500,\n",
    "                 activationFunction = SigmoidActivationFunction, costFunction = CrossEntropyCostFunction):\n",
    "        self._reg_lambda = reg_lambda\n",
    "        self.epsilon_init = epsilon_init\n",
    "        self.hidden_layer_size1 = hidden_layer_size1\n",
    "        self.hidden_layer_size2 = hidden_layer_size2\n",
    "        self._method = opti_method\n",
    "        self._maxiter = maxiter\n",
    "        self._costFunction = costFunction\n",
    "        self._activationFunction = activationFunction\n",
    "        self._layerSizes = layerSizes\n",
    "    \n",
    "    def rand_init(self, l_in, l_out):\n",
    "        return np.random.randn(l_out, l_in + 1) * 2 * self.epsilon_init - self.epsilon_init\n",
    "    \n",
    "    def packWeights(self, weightsList):\n",
    "        return np.concatenate([w.ravel() for w in weightsList])\n",
    "    \n",
    "    def unpackWeightsAlgorithm(self, weights, layerSizes):\n",
    "        requredLen = sum([y*x for y,x in layerSizes])\n",
    "        if requredLen == len(weights):\n",
    "            start = 0\n",
    "            returnList = []\n",
    "            for y,x in layerSizes:\n",
    "                returnList.append(weights[start:start+y*x].reshape((y,x)))\n",
    "                start +=y*x\n",
    "            return returnList\n",
    "        else:\n",
    "            print(\"Weights sizes mismatch,\", requredLen, \"weights requred\")\n",
    "            \n",
    "    def unpackWeights(self, thetas, input_layer_size, hidden_layer_size1, num_labels):\n",
    "        ls = self.unpackWeightsAlgorithm(thetas, [(hidden_layer_size1, input_layer_size + 1),(num_labels, hidden_layer_size1 + 1)])\n",
    "        return ls[0], ls[1]\n",
    "    \n",
    "    def _forward(self, X, t1, t2):        \n",
    "        a1 = self.addOnes(X)\n",
    "\n",
    "        z2 = np.dot(t1, a1.T)\n",
    "        a2 = self._activationFunction.value(z2)\n",
    "        a2 = self.addOnes(a2.T)\n",
    "        \n",
    "        \n",
    "        z3 = np.dot(t2, a2.T)\n",
    "        a3 = self._activationFunction.value(z3)\n",
    "        \n",
    "        \n",
    "        return a1, z2, a2, z3, a3\n",
    "    \n",
    "    \n",
    "    def variableSetup(self, thetas, input_layer_size, hidden_layer_size1, num_labels, X, y):\n",
    "        t1, t2 = self.unpackWeights(thetas, input_layer_size, hidden_layer_size1, num_labels)\n",
    "        m = X.shape[0]\n",
    "        Y = np.eye(num_labels)[y]\n",
    "        return (t1, t2, m, Y)\n",
    "        \n",
    "    def removeBiasesFromWeightMatrices(self, listOfWeights):\n",
    "        return [w[:, 1:] for w in listOfWeights]\n",
    "    \n",
    "    def function(self, thetas, input_layer_size, hidden_layer_size1, num_labels, X, y, reg_lambda):\n",
    "        t1, t2, m, Y = self.variableSetup(thetas, input_layer_size, hidden_layer_size1, num_labels, X, y)\n",
    "        \n",
    "        _, _, _, _, h = self._forward(X, t1, t2)\n",
    "        \n",
    "        J = self._costFunction.cost(Y, h, m)\n",
    "        reg = self._costFunction.regulazation(self.removeBiasesFromWeightMatrices([t1, t2]), self._reg_lambda, m)\n",
    "        return J + reg\n",
    "    \n",
    "    def addOnes(self, x):\n",
    "        ys,xs = x.shape\n",
    "        z = np.ones((ys,1))\n",
    "        return np.concatenate((z, x), axis=1)\n",
    "        \n",
    "    def function_prime(self, thetas, input_layer_size, hidden_layer_size1, num_labels, X, y, reg_lambda):\n",
    "        t1, t2, m, Y = self.variableSetup(thetas, input_layer_size, hidden_layer_size1, num_labels, X, y)\n",
    "\n",
    "        t1f = t1[:, 1:]\n",
    "        t2f = t2[:, 1:]\n",
    "\n",
    "        a1, z2, a2, z3, a3 = self._forward(X, t1, t2)\n",
    "        \n",
    "        si3 = self._costFunction.delta(Y, a3.T, self._activationFunction)\n",
    "        si2 = (np.dot(si3, t2) * self._activationFunction.derivative(self.addOnes(z2.T)))[:, 1:]\n",
    "\n",
    "        d1 = np.dot(si2.T,a1);\n",
    "        d2 = np.dot(si3.T,a2);  \n",
    "        \n",
    "        Theta1_grad = d1 / m\n",
    "        Theta2_grad = d2 / m\n",
    "        \n",
    "        if reg_lambda != 0:\n",
    "            Theta1_grad[:, 1:] += (reg_lambda / m) * t1f\n",
    "            Theta2_grad[:, 1:] += (reg_lambda / m) * t2f\n",
    "        \n",
    "        return self.packWeights([Theta1_grad, Theta2_grad])\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        num_features = X.shape[0]\n",
    "        input_layer_size = X.shape[1]\n",
    "        num_labels = len(set(y))\n",
    "        \n",
    "        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size1)\n",
    "        theta2_0 = self.rand_init(self.hidden_layer_size1, num_labels)\n",
    "        \n",
    "        thetas0 = self.packWeights([theta1_0, theta2_0])\n",
    "        \n",
    "        options = {'maxiter': self._maxiter}\n",
    "        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self._method, \n",
    "                                 args=(input_layer_size, self.hidden_layer_size1, num_labels, X, y, self._reg_lambda), options=options)\n",
    "        \n",
    "        self.t1, self.t2 = self.unpackWeights(_res.x, input_layer_size, self.hidden_layer_size1, num_labels)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).argmax(0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        _, _, _, _, h = self._forward(X, self.t1, self.t2)\n",
    "        return h\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92733333333333334"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "data = loadmat('ex3data1.mat')\n",
    "np.random.seed(40)\n",
    "X, y = data['X'], data['y']\n",
    "y = y.reshape(X.shape[0])\n",
    "y = y - 1  # Fix notation # TODO: Automaticlly fix that on the class\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "nn = NN_1HL(layerSizes=[400, 25,10], maxiter=300, reg_lambda=3)\n",
    "\n",
    "nn.fit(X_train, y_train)\n",
    "\n",
    "accuracy_score(y_test, nn.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9273333333333333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.92733333333333334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.84155574  0.16183435]\n",
      " [ 0.00990963  0.57962061]\n",
      " [ 0.73230111  0.33887796]]\n",
      "\n",
      "[[ 0.40263826  0.63121061  0.6092633   0.42682653]\n",
      " [ 0.10565036  0.45338619  0.29942417  0.57054768]]\n",
      "\n",
      "[[ 0.33844838  0.5150509 ]\n",
      " [ 0.97271401  0.93198127]\n",
      " [ 0.18242523  0.20874683]\n",
      " [ 0.40828759  0.87006715]\n",
      " [ 0.80329704  0.93840606]]\n",
      "\n",
      "[ 0.84155574  0.16183435  0.00990963  0.57962061  0.73230111  0.33887796\n",
      "  0.40263826  0.63121061  0.6092633   0.42682653  0.10565036  0.45338619\n",
      "  0.29942417  0.57054768  0.33844838  0.5150509   0.97271401  0.93198127\n",
      "  0.18242523  0.20874683  0.40828759  0.87006715  0.80329704  0.93840606]\n",
      "\n",
      "[[ 0.84155574  0.16183435]\n",
      " [ 0.00990963  0.57962061]\n",
      " [ 0.73230111  0.33887796]]\n",
      "[[ 0.40263826  0.63121061  0.6092633   0.42682653]\n",
      " [ 0.10565036  0.45338619  0.29942417  0.57054768]]\n",
      "[[ 0.33844838  0.5150509 ]\n",
      " [ 0.97271401  0.93198127]\n",
      " [ 0.18242523  0.20874683]\n",
      " [ 0.40828759  0.87006715]\n",
      " [ 0.80329704  0.93840606]]\n"
     ]
    }
   ],
   "source": [
    "w1 = np.random.rand(3,2)\n",
    "w2 = np.random.rand(2,4)\n",
    "w3 = np.random.rand(5,2)\n",
    "print(w1)\n",
    "print(\"\")\n",
    "print(w2)\n",
    "print(\"\")\n",
    "print(w3)\n",
    "print(\"\")\n",
    "x = np.concatenate([w1.ravel(),w2.ravel(),w3.ravel()])\n",
    "print(x)\n",
    "\n",
    "sizes = [(3,2),(2,4),(5,2)]\n",
    "print(\"\")\n",
    "def neki(sizes, weights):\n",
    "    if sum([y*x for y,x in sizes]) == len(weights):\n",
    "        start = 0\n",
    "        for y,x in sizes:\n",
    "            print(weights[start:start+y*x].reshape((y,x)))\n",
    "            start +=y*x\n",
    "    else:\n",
    "        print(\"error\")\n",
    "    \n",
    "\n",
    "neki(sizes, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [x for x in range(0,10)]\n",
    "l[:-1]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
