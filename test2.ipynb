{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SigmoidActivationFunction:\n",
    "    \n",
    "    @staticmethod\n",
    "    def value(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        sig = SigmoidActivationFunction.value(z)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "class WeightPacking:\n",
    "    @staticmethod\n",
    "    def pack(weights, biases):\n",
    "        return np.concatenate([np.concatenate([w.ravel(), b.ravel()]) for w,b in zip(weights, biases)])\n",
    "    \n",
    "    @staticmethod\n",
    "    def unpack(thetas, layers):\n",
    "        start = 0\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for h in range(len(layers)-1):\n",
    "            y,x = layers[h], layers[h+1]\n",
    "            weights.append(thetas[start:start+y*x].reshape(y,x))\n",
    "            start += y*x\n",
    "            biases.append(thetas[start:start+x].reshape(1, x))\n",
    "            start += x\n",
    "        return (weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NN_1HL(object):\n",
    "    \n",
    "    def __init__(self, reg_lambda=0, epsilon_init=0.12,\n",
    "                 hidden_layer_size=25, opti_method='TNC', maxiter=500, ActivationFunction = SigmoidActivationFunction):\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.epsilon_init = epsilon_init\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.activation_func = ActivationFunction.value\n",
    "        self.activation_func_prime = ActivationFunction.derivative\n",
    "        self.method = opti_method\n",
    "        self.maxiter = maxiter\n",
    "    \n",
    "    def rand_init(self, l_in, l_out):\n",
    "        return np.random.rand(l_out, l_in + 1) * 2 * self.epsilon_init - self.epsilon_init\n",
    "    \n",
    "    def pack_thetas(self, t1, t2):\n",
    "        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))\n",
    "    \n",
    "    def unpack_thetas(self, thetas, layers):\n",
    "        \n",
    "        input_layer_size, hidden_layer_size, num_labels = layers[0], layers[1], layers[2]\n",
    "        \n",
    "        t1_start = 0\n",
    "        t1_end = hidden_layer_size * (input_layer_size + 1)\n",
    "        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))\n",
    "        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))\n",
    "        return t1, t2\n",
    "    \n",
    "\n",
    "    def costCalc(self, actual, predicted):\n",
    "        costPositive = -actual * np.log(predicted)\n",
    "        costNegative = (1 - actual) * np.log(1 - predicted)\n",
    "        cost = costPositive - costNegative\n",
    "        return np.sum(cost)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).argmax(0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        t1w = self.t1[:, 1:].T\n",
    "        t1b = self.t1[:, 0][np.newaxis]\n",
    "\n",
    "        t2w = self.t2[:, 1:].T\n",
    "        t2b = self.t2[:, 0][np.newaxis]\n",
    "        \n",
    "        _, _, _, _, h = self._forward(X, [t1w, t2w], [t1b, t2b])\n",
    "        return h.T\n",
    "    \n",
    "    def _forward(self, X, weights, biases):\n",
    "       \n",
    "        a1new = X\n",
    "        z1new = np.dot(X, weights[0]) + biases[0]\n",
    "        a2new = self.activation_func(z1new)\n",
    "        z2new = np.dot(a2new, weights[1]) + biases[1]\n",
    "        a3new = self.activation_func(z2new)\n",
    "        \n",
    "        yd, xd = z1new.T.shape\n",
    "        if xd == 1:\n",
    "            z1new =  z1new.ravel()\n",
    "            z2new =  z2new.ravel()\n",
    "        \n",
    "        return a1new, z1new, a2new, z2new, a3new    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        num_features = X.shape[0]\n",
    "        input_layer_size = X.shape[1]\n",
    "        num_labels = len(set(y))\n",
    "        \n",
    "        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)\n",
    "        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)\n",
    "        \n",
    "        \n",
    "        layers = [input_layer_size, self.hidden_layer_size, num_labels]\n",
    "        \n",
    "        t1w = theta1_0[:, 1:].T\n",
    "        t1b = theta1_0[:, 0][np.newaxis]\n",
    "\n",
    "        t2w = theta2_0[:, 1:].T\n",
    "        t2b = theta2_0[:, 0][np.newaxis]\n",
    "        \n",
    "        print(\"initw\", t1w)\n",
    "        print(\"initb\", t1b)\n",
    "\n",
    "        \n",
    "        thetas0 = WeightPacking.pack([t1w, t2w], [t1b, t2b])\n",
    "        #print(thetas0)\n",
    "        options = {'maxiter': self.maxiter}\n",
    "        _res = optimize.minimize(self.function, thetas0, jac=True, method=self.method, \n",
    "                                 args=(layers, X, y, self.reg_lambda), options=options)\n",
    "        \n",
    "        self.t1, self.t2 = self.unpack_thetas(_res.x, [input_layer_size, self.hidden_layer_size, num_labels])\n",
    "    \n",
    "    \n",
    "    def function(self, thetas, layers, X, y, reg_lambda):\n",
    "\n",
    "        print(\"1 vector\", thetas)\n",
    "        \n",
    "        t1, t2 = self.unpack_thetas(thetas, layers)\n",
    "        m = X.shape[0]\n",
    "\n",
    "        wk,bk = WeightPacking.unpack(thetas, layers)\n",
    "\n",
    "        #print(t1 == wk[0])\n",
    "        \n",
    "        t1w = t1[:, 1:].T\n",
    "        t1b = t1[:, 0][np.newaxis]\n",
    "\n",
    "        t2w = t2[:, 1:].T\n",
    "        t2b = t2[:, 0][np.newaxis]\n",
    "        \n",
    "        print(\"inf1\", t1w)\n",
    "        print(\"inf2\", wk[0])\n",
    "        print(\"inf2b\", bk[0])\n",
    "        \n",
    "        Y = np.eye(layers[-1])[y]\n",
    "        \n",
    "\n",
    "        _, _, _, _, h = self._forward(X, [t1w, t2w], [t1b, t2b])\n",
    "\n",
    "        J = self.costCalc(Y, h) / m\n",
    "        \n",
    "        if reg_lambda != 0:\n",
    "            t1f = t1[:, 1:]\n",
    "            t2f = t2[:, 1:]\n",
    "            J += self.reg_lambda / (2 * m) * (np.sum(t1f**2) + np.sum(t2f**2))\n",
    "        \n",
    "\n",
    "        b1 = np.zeros(layers[1])\n",
    "        b2 = np.zeros(layers[2])\n",
    "        \n",
    "        Delta1, Delta2 = np.zeros(t1w.shape), np.zeros(t2w.shape)\n",
    "\n",
    "        for i, row in enumerate(X):\n",
    "            \n",
    "            a1, z2, a2, z3, a3 = self._forward(row, [t1w, t2w], [t1b, t2b])\n",
    "\n",
    "            # Backprop\n",
    "            d3 = (a3 - Y[i, :]).T\n",
    "            d2 = np.dot(t2w, d3).ravel() * self.activation_func_prime(z2)\n",
    "            \n",
    "            b2+=d3.ravel()\n",
    "            b1+=d2.ravel()\n",
    "            \n",
    "            Delta2 += np.dot(d3, a2).T\n",
    "            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis]).T\n",
    "            \n",
    "            \n",
    "        Theta1_grad = np.zeros(t1.shape)\n",
    "        Theta1_grad[:, 1:] = Delta1.T/m\n",
    "        \n",
    "        Theta2_grad = np.zeros(t2.shape)\n",
    "        Theta2_grad[:, 1:] = Delta2.T/m\n",
    "        \n",
    "        \n",
    "        Theta1_grad[:, 0] = b1/m\n",
    "        Theta2_grad[:, 0] = b2/m\n",
    "        \n",
    "        \n",
    "        if reg_lambda != 0:\n",
    "            Theta1_grad[:, 1:] += (reg_lambda / m) * t1w.T\n",
    "            Theta2_grad[:, 1:] += (reg_lambda / m) * t2w.T\n",
    "        #return J,  WeightPacking.pack([Theta1_grad[:, 1:].T, Theta2_grad[:, 1:].T], [Theta1_grad[:, 0], Theta2_grad[:, 0]])\n",
    "        return (J, self.pack_thetas(Theta1_grad, Theta2_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initw [[  7.38225654e-02   2.37581880e-02  -1.19897599e-01  -1.83089681e-02\n",
      "   -1.18595110e-01  -9.19972694e-02   5.61109879e-02  -2.06488327e-02\n",
      "   -7.78579096e-03   4.57200015e-02   3.87457960e-02  -5.00563207e-02\n",
      "    6.02613223e-03   6.48767632e-03   7.07117426e-02   4.24365587e-02\n",
      "   -9.89764294e-02  -5.84824875e-02  -1.00530324e-01   9.20848883e-02\n",
      "    7.76363847e-02   4.54247214e-02  -1.31462299e-02   8.38567137e-02\n",
      "   -5.68046740e-02]\n",
      " [  1.12890583e-01  -5.79564105e-02  -7.06541164e-02  -5.00154553e-02\n",
      "   -1.00125214e-01   7.51773483e-02  -1.07830517e-01  -5.59744100e-02\n",
      "   -2.59859136e-02  -9.04573505e-02  -6.90088422e-02   1.14735601e-01\n",
      "    9.43104685e-02   6.09670170e-02   5.70721206e-02   9.90260589e-02\n",
      "    4.97579875e-02  -4.75046717e-02  -1.12650885e-01  -3.74048895e-02\n",
      "    1.57641153e-02   2.81984826e-02   1.18767329e-01   4.20378353e-02\n",
      "    7.83467370e-02]\n",
      " [  6.95360574e-02  -2.83411234e-02  -2.96864952e-02   4.21914269e-02\n",
      "    4.04663327e-03   1.00650895e-01  -5.10863259e-02  -8.59401544e-02\n",
      "   -4.44693058e-02   2.54273291e-02   3.24545747e-02  -4.16625914e-02\n",
      "    6.67633762e-02   7.27816640e-02  -5.71912922e-02  -1.30518673e-02\n",
      "   -2.24499608e-02  -1.17686142e-01   7.36026940e-02  -2.14031979e-02\n",
      "    1.66453324e-02   9.80683590e-02  -1.17739493e-01  -9.90475275e-03\n",
      "    5.21933020e-02]\n",
      " [  2.89874594e-02   2.27951102e-02  -8.22192656e-05   2.14531077e-02\n",
      "   -5.45735283e-02   1.18145981e-01   7.62998959e-02   4.97675578e-02\n",
      "   -3.94236397e-02   8.62291913e-02  -5.77976060e-02  -9.11791141e-02\n",
      "   -9.58761653e-02   9.46223859e-02   5.74679964e-02  -3.97784202e-02\n",
      "    1.84470450e-02  -6.62744071e-02  -3.64370635e-02   1.16720061e-01\n",
      "   -6.90422808e-02  -1.12249013e-01  -1.12247808e-01  -7.73388523e-02\n",
      "    5.41117643e-02]]\n",
      "initb [[ 0.06294601 -0.10816577 -0.05890349 -0.01506513  0.0846184   0.00842498\n",
      "  -0.06197767 -0.01003916 -0.02053278 -0.10260059 -0.11688671 -0.10598927\n",
      "   0.00062708 -0.06489656 -0.08828632  0.06415867  0.05545336 -0.06136951\n",
      "  -0.02524087 -0.11931    -0.04588201 -0.02318725 -0.07137006  0.03568746\n",
      "  -0.00733838]]\n",
      "1 vector [  7.38225654e-02   2.37581880e-02  -1.19897599e-01  -1.83089681e-02\n",
      "  -1.18595110e-01  -9.19972694e-02   5.61109879e-02  -2.06488327e-02\n",
      "  -7.78579096e-03   4.57200015e-02   3.87457960e-02  -5.00563207e-02\n",
      "   6.02613223e-03   6.48767632e-03   7.07117426e-02   4.24365587e-02\n",
      "  -9.89764294e-02  -5.84824875e-02  -1.00530324e-01   9.20848883e-02\n",
      "   7.76363847e-02   4.54247214e-02  -1.31462299e-02   8.38567137e-02\n",
      "  -5.68046740e-02   1.12890583e-01  -5.79564105e-02  -7.06541164e-02\n",
      "  -5.00154553e-02  -1.00125214e-01   7.51773483e-02  -1.07830517e-01\n",
      "  -5.59744100e-02  -2.59859136e-02  -9.04573505e-02  -6.90088422e-02\n",
      "   1.14735601e-01   9.43104685e-02   6.09670170e-02   5.70721206e-02\n",
      "   9.90260589e-02   4.97579875e-02  -4.75046717e-02  -1.12650885e-01\n",
      "  -3.74048895e-02   1.57641153e-02   2.81984826e-02   1.18767329e-01\n",
      "   4.20378353e-02   7.83467370e-02   6.95360574e-02  -2.83411234e-02\n",
      "  -2.96864952e-02   4.21914269e-02   4.04663327e-03   1.00650895e-01\n",
      "  -5.10863259e-02  -8.59401544e-02  -4.44693058e-02   2.54273291e-02\n",
      "   3.24545747e-02  -4.16625914e-02   6.67633762e-02   7.27816640e-02\n",
      "  -5.71912922e-02  -1.30518673e-02  -2.24499608e-02  -1.17686142e-01\n",
      "   7.36026940e-02  -2.14031979e-02   1.66453324e-02   9.80683590e-02\n",
      "  -1.17739493e-01  -9.90475275e-03   5.21933020e-02   2.89874594e-02\n",
      "   2.27951102e-02  -8.22192656e-05   2.14531077e-02  -5.45735283e-02\n",
      "   1.18145981e-01   7.62998959e-02   4.97675578e-02  -3.94236397e-02\n",
      "   8.62291913e-02  -5.77976060e-02  -9.11791141e-02  -9.58761653e-02\n",
      "   9.46223859e-02   5.74679964e-02  -3.97784202e-02   1.84470450e-02\n",
      "  -6.62744071e-02  -3.64370635e-02   1.16720061e-01  -6.90422808e-02\n",
      "  -1.12249013e-01  -1.12247808e-01  -7.73388523e-02   5.41117643e-02\n",
      "   6.29460056e-02  -1.08165766e-01  -5.89034916e-02  -1.50651268e-02\n",
      "   8.46184012e-02   8.42498455e-03  -6.19776750e-02  -1.00391573e-02\n",
      "  -2.05327788e-02  -1.02600594e-01  -1.16886712e-01  -1.05989269e-01\n",
      "   6.27078623e-04  -6.48965567e-02  -8.82863169e-02   6.41586707e-02\n",
      "   5.54533641e-02  -6.13695076e-02  -2.52408685e-02  -1.19309998e-01\n",
      "  -4.58820133e-02  -2.31872460e-02  -7.13700589e-02   3.56874607e-02\n",
      "  -7.33838444e-03   5.68378129e-02  -6.53251601e-02  -1.17275495e-03\n",
      "   6.26221859e-02   3.34989109e-02  -7.09132790e-02  -7.94435405e-02\n",
      "   1.10878462e-01  -1.86213884e-03  -2.76221708e-02   6.25984713e-03\n",
      "  -1.37226040e-02   2.12573152e-02  -5.06459987e-02  -4.52468163e-02\n",
      "   3.74136303e-02   1.10760229e-01   1.47679494e-02   1.05036014e-02\n",
      "   2.76013346e-02   3.21867962e-03  -1.16693964e-02   3.34526589e-02\n",
      "  -1.33119169e-02   6.99686342e-03   2.43511969e-02  -1.06438340e-01\n",
      "  -9.05529315e-02  -5.21566362e-03  -7.29056718e-02  -4.38431927e-02\n",
      "   1.40499678e-02   3.59289308e-02   7.77999364e-02  -5.32680779e-02\n",
      "   6.52979158e-02  -7.15335294e-02  -1.18680724e-01  -2.11438275e-02\n",
      "   6.76179050e-04  -2.35852854e-02  -1.06691229e-01  -3.14837528e-02\n",
      "  -4.33243110e-02  -8.00972496e-02   8.38135963e-02  -3.05740180e-02\n",
      "   1.14996078e-01  -1.00766740e-01  -3.79408578e-03  -7.78946804e-02\n",
      "  -1.15280974e-01  -1.12121120e-01   2.81540841e-02   7.96932360e-02\n",
      "   8.30409220e-03   1.15475157e-01   2.04685741e-02   4.40140197e-02\n",
      "  -1.37399187e-02   8.24064367e-02   3.12604886e-02  -1.18435664e-01\n",
      "   4.75050536e-02  -8.61384628e-02   2.74390434e-02   3.19923156e-02\n",
      "  -3.84416564e-02  -1.17807063e-03   2.27832066e-02  -9.95561294e-02\n",
      "  -2.47232337e-02  -8.07843178e-02  -5.93455867e-02  -7.69669348e-02\n",
      "  -9.43299550e-03  -1.40429567e-02  -9.18914499e-02]\n",
      "inf1 [[  2.37581880e-02   5.61109879e-02  -5.00563207e-02  -9.89764294e-02\n",
      "    4.54247214e-02  -5.79564105e-02  -1.07830517e-01   1.14735601e-01\n",
      "    4.97579875e-02   2.81984826e-02  -2.83411234e-02  -5.10863259e-02\n",
      "   -4.16625914e-02  -2.24499608e-02   9.80683590e-02   2.27951102e-02\n",
      "    7.62998959e-02  -9.11791141e-02   1.84470450e-02  -1.12249013e-01\n",
      "   -1.08165766e-01  -6.19776750e-02  -1.05989269e-01   5.54533641e-02\n",
      "   -2.31872460e-02]\n",
      " [ -1.19897599e-01  -2.06488327e-02   6.02613223e-03  -5.84824875e-02\n",
      "   -1.31462299e-02  -7.06541164e-02  -5.59744100e-02   9.43104685e-02\n",
      "   -4.75046717e-02   1.18767329e-01  -2.96864952e-02  -8.59401544e-02\n",
      "    6.67633762e-02  -1.17686142e-01  -1.17739493e-01  -8.22192656e-05\n",
      "    4.97675578e-02  -9.58761653e-02  -6.62744071e-02  -1.12247808e-01\n",
      "   -5.89034916e-02  -1.00391573e-02   6.27078623e-04  -6.13695076e-02\n",
      "   -7.13700589e-02]\n",
      " [ -1.83089681e-02  -7.78579096e-03   6.48767632e-03  -1.00530324e-01\n",
      "    8.38567137e-02  -5.00154553e-02  -2.59859136e-02   6.09670170e-02\n",
      "   -1.12650885e-01   4.20378353e-02   4.21914269e-02  -4.44693058e-02\n",
      "    7.27816640e-02   7.36026940e-02  -9.90475275e-03   2.14531077e-02\n",
      "   -3.94236397e-02   9.46223859e-02  -3.64370635e-02  -7.73388523e-02\n",
      "   -1.50651268e-02  -2.05327788e-02  -6.48965567e-02  -2.52408685e-02\n",
      "    3.56874607e-02]\n",
      " [ -1.18595110e-01   4.57200015e-02   7.07117426e-02   9.20848883e-02\n",
      "   -5.68046740e-02  -1.00125214e-01  -9.04573505e-02   5.70721206e-02\n",
      "   -3.74048895e-02   7.83467370e-02   4.04663327e-03   2.54273291e-02\n",
      "   -5.71912922e-02  -2.14031979e-02   5.21933020e-02  -5.45735283e-02\n",
      "    8.62291913e-02   5.74679964e-02   1.16720061e-01   5.41117643e-02\n",
      "    8.46184012e-02  -1.02600594e-01  -8.82863169e-02  -1.19309998e-01\n",
      "   -7.33838444e-03]]\n",
      "inf2 [[  7.38225654e-02   2.37581880e-02  -1.19897599e-01  -1.83089681e-02\n",
      "   -1.18595110e-01  -9.19972694e-02   5.61109879e-02  -2.06488327e-02\n",
      "   -7.78579096e-03   4.57200015e-02   3.87457960e-02  -5.00563207e-02\n",
      "    6.02613223e-03   6.48767632e-03   7.07117426e-02   4.24365587e-02\n",
      "   -9.89764294e-02  -5.84824875e-02  -1.00530324e-01   9.20848883e-02\n",
      "    7.76363847e-02   4.54247214e-02  -1.31462299e-02   8.38567137e-02\n",
      "   -5.68046740e-02]\n",
      " [  1.12890583e-01  -5.79564105e-02  -7.06541164e-02  -5.00154553e-02\n",
      "   -1.00125214e-01   7.51773483e-02  -1.07830517e-01  -5.59744100e-02\n",
      "   -2.59859136e-02  -9.04573505e-02  -6.90088422e-02   1.14735601e-01\n",
      "    9.43104685e-02   6.09670170e-02   5.70721206e-02   9.90260589e-02\n",
      "    4.97579875e-02  -4.75046717e-02  -1.12650885e-01  -3.74048895e-02\n",
      "    1.57641153e-02   2.81984826e-02   1.18767329e-01   4.20378353e-02\n",
      "    7.83467370e-02]\n",
      " [  6.95360574e-02  -2.83411234e-02  -2.96864952e-02   4.21914269e-02\n",
      "    4.04663327e-03   1.00650895e-01  -5.10863259e-02  -8.59401544e-02\n",
      "   -4.44693058e-02   2.54273291e-02   3.24545747e-02  -4.16625914e-02\n",
      "    6.67633762e-02   7.27816640e-02  -5.71912922e-02  -1.30518673e-02\n",
      "   -2.24499608e-02  -1.17686142e-01   7.36026940e-02  -2.14031979e-02\n",
      "    1.66453324e-02   9.80683590e-02  -1.17739493e-01  -9.90475275e-03\n",
      "    5.21933020e-02]\n",
      " [  2.89874594e-02   2.27951102e-02  -8.22192656e-05   2.14531077e-02\n",
      "   -5.45735283e-02   1.18145981e-01   7.62998959e-02   4.97675578e-02\n",
      "   -3.94236397e-02   8.62291913e-02  -5.77976060e-02  -9.11791141e-02\n",
      "   -9.58761653e-02   9.46223859e-02   5.74679964e-02  -3.97784202e-02\n",
      "    1.84470450e-02  -6.62744071e-02  -3.64370635e-02   1.16720061e-01\n",
      "   -6.90422808e-02  -1.12249013e-01  -1.12247808e-01  -7.73388523e-02\n",
      "    5.41117643e-02]]\n",
      "inf2b [[ 0.06294601 -0.10816577 -0.05890349 -0.01506513  0.0846184   0.00842498\n",
      "  -0.06197767 -0.01003916 -0.02053278 -0.10260059 -0.11688671 -0.10598927\n",
      "   0.00062708 -0.06489656 -0.08828632  0.06415867  0.05545336 -0.06136951\n",
      "  -0.02524087 -0.11931    -0.04588201 -0.02318725 -0.07137006  0.03568746\n",
      "  -0.00733838]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33333333333333331"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(40)\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn import cross_validation\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "nn = NN_1HL(reg_lambda = 2.1, maxiter=0)\n",
    "nn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, nn.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.96666666666666667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initw [[ 0.10016364 -0.04572237 -0.07647824 ..., -0.09294501 -0.1027607\n",
      "   0.01316554]\n",
      " [-0.03847699 -0.07536525  0.03132587 ..., -0.07037281  0.11270158\n",
      "  -0.10070193]\n",
      " [-0.06391403 -0.01410597 -0.05743196 ...,  0.01684863  0.10433352\n",
      "   0.10846168]\n",
      " ..., \n",
      " [ 0.04227835 -0.03933255  0.11128212 ..., -0.10972296 -0.05346367\n",
      "  -0.1055018 ]\n",
      " [-0.10586833 -0.01503455 -0.04825858 ..., -0.00073086  0.01250652\n",
      "   0.06834749]\n",
      " [-0.04589032  0.10956645 -0.00678537 ..., -0.11422528  0.01886324\n",
      "   0.09208523]]\n",
      "initb [[ 0.05055243  0.06222448 -0.11862853 -0.0596204  -0.01544234 -0.11404066\n",
      "   0.02283608 -0.07185854  0.0884596   0.07929232  0.0893194   0.07857383\n",
      "  -0.06593219 -0.02185787  0.07096474  0.02023183 -0.11116568  0.01329271\n",
      "   0.06076685 -0.0133218  -0.0678304   0.08921589 -0.01858226  0.01051906\n",
      "  -0.08535013]]\n",
      "[ 0.10016364 -0.04572237 -0.07647824 ..., -0.11804889  0.05393864\n",
      "  0.07469002]\n",
      "[ 0.10016364 -0.04572237 -0.07647824 ..., -0.11804889  0.05393864\n",
      "  0.07469002]\n",
      "[ 0.10016364 -0.04572237 -0.07647824 ..., -0.11804889  0.05393864\n",
      "  0.07469002]\n",
      "[ 0.10016364 -0.04572237 -0.07647824 ..., -0.11804889  0.05393864\n",
      "  0.07469002]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10299999999999999"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(40)\n",
    "from scipy.io import loadmat\n",
    "data = loadmat('ex3data1.mat')\n",
    "X, y = data['X'], data['y']\n",
    "y = y.reshape(X.shape[0], )\n",
    "y = y - 1  # Fix notation # TODO: Automaticlly fix that on the class\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "\n",
    "nn = NN_1HL(maxiter=50, reg_lambda = 2.1)\n",
    "nn.fit(X_train, y_train)\n",
    "\n",
    "accuracy_score(y_test, nn.predict(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.876"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#class NeuralNet:\n",
    "#    def __init__(self, reg_lambda=0, epsilon_init=0.12, hiddenLayerSizes = [25],  opti_method='TNC', maxiter=500):\n",
    "#        self.reg_lambda = reg_lambda\n",
    "#        self.epsilon_init = epsilon_init\n",
    "#        self.method = opti_method\n",
    "#        self.maxiter = maxiter\n",
    "#        self._hiddenLayerSizes = hiddenLayerSizes\n",
    "#        \n",
    "#    def weightsInit(self, layers):\n",
    "#        weights = []\n",
    "#        biases = []\n",
    "#        for x in range(len(layers)-1):\n",
    "#            weights.append(np.random.randn(layers[x], layers[x+1]))\n",
    "#            biases.append(np.random.randn(1, layers[x+1]))\n",
    "#        return weights, biases\n",
    "#        \n",
    "#    def fit(self, X, y):\n",
    "#        #print(\"yxdfs\", X.shape)\n",
    "#        numFeatures = X.shape[0]\n",
    "#        inputLayerSize = X.shape[1]\n",
    "#        numLabels = len(set(y))\n",
    "#        \n",
    "#        layers = [inputLayerSize] + self._hiddenLayerSizes + [numLabels]\n",
    "#        weights, biases = self.weightsInit(layers)\n",
    "#        \n",
    "#        \n",
    "#        #for w, b in zip(weights, biases):\n",
    "#        #    print(w.shape, b.shape)\n",
    "#        \n",
    "#        p = self.pack(weights, biases)\n",
    "#        #wt, bt = self.unpack(p, layers)\n",
    "#        #\n",
    "#        #for w, b in zip(wt, bt):\n",
    "#        #    print(w.shape, b.shape)\n",
    "#        \n",
    "#        options = {'maxiter': 500}\n",
    "#        _res = optimize.minimize(self.optimFunction, p, jac=True, method='TNC', \n",
    "#                                 args=(layers, X, y, 0), options=options)\n",
    "#        \n",
    "#        \n",
    "#        \n",
    "#        \n",
    "#        self.optimFunction(_res.x, layers, X, y, 0)\n",
    "#        \n",
    "#        self._weights, self._biases = weights, biases\n",
    "#        \n",
    "\n",
    "#            \n",
    "#        \n",
    "\n",
    "#\n",
    "#\n",
    "#        \n",
    "#    def optimFunction(self, thetas, layers,  X, y, reg_lambda):\n",
    "#        weights, biases = self.unpack(thetas, layers)\n",
    "#        \n",
    "#        m = X.shape[0]\n",
    "#        wt, bt = self.unpack(thetas, layers)\n",
    "#        Y = np.eye(layers[-1])[y]\n",
    "#        \n",
    "#\n",
    "#        predicted = self._forward(X, weights, biases)\n",
    "#        \n",
    "#        #J = np.sum(np.nan_to_num(-Y * np.log(predicted) - (1 - Y) * np.log(1 - predicted))) / m\n",
    "#        #print(J)\n",
    "#        \n",
    "#        nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "#        nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "#        \n",
    "#        for i, row in enumerate(X):\n",
    "#            \n",
    "#            \n",
    "#            #Forward\n",
    "#            inputActivations = [X[i,:]]\n",
    "#            outputActivations = []\n",
    "#            for w,b in zip(weights, biases):\n",
    "#                z = np.dot(inputActivations[-1], w)+b\n",
    "#                outputActivations.append(z)\n",
    "#                inputActivations.append(self.sigmoid(z))\n",
    "#            predicted = inputActivations[-1]\n",
    "#            \n",
    "#            #Back\n",
    "#            ds = [predicted - Y[i, :]]\n",
    "#            for x in range(len(weights)-1):\n",
    "#                ds.append(np.dot(ds[-1], weights[1].T) * self.sigmoid_prime(outputActivations[0]))\n",
    "#               \n",
    "#            \n",
    "#            \n",
    "#            nabla_b = [d+b for b, d in zip(reversed(ds), nabla_b)]\n",
    "#            \n",
    "#            #print(inputActivations[0].shape)\n",
    "#            \n",
    "#            Delta2 = np.dot(inputActivations[1].T, ds[0])\n",
    "#            Delta1 = np.dot(inputActivations[0][np.newaxis].T, ds[1])\n",
    "#            \n",
    "#            nabla_w = [nabla_w[0] + Delta1, nabla_w[1] + Delta2]\n",
    "#            \n",
    "#            \n",
    "#        Thetagrad = [w/m for w in nabla_w]\n",
    "#        Thetagradbias = [b/m for b in  nabla_b]\n",
    "#        \n",
    "#        return (J, self.pack(Thetagrad, Thetagradbias))\n",
    "#            \n",
    "#        \n",
    "#        \n",
    "#        \n",
    "#        \n",
    "#    def sigmoid(self, X):\n",
    "#        return 1 / (1 + np.exp(-X))\n",
    "#    \n",
    "#    def sigmoid_prime(self, z):\n",
    "#        sig = self.sigmoid(z)\n",
    "#        return sig * (1 - sig)\n",
    "#        \n",
    "#    def _forward(self, X, weights, biases):        \n",
    "#        for w, b in zip(weights, biases):\n",
    "#            X = self.sigmoid(np.dot(X, w) + b)\n",
    "#        return X\n",
    "#    \n",
    "#    def predict(self, X):\n",
    "#        return self._forward(X, self._weights, self._biases).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.random.seed(40)\n",
    "#import sklearn.datasets as datasets\n",
    "#from sklearn import cross_validation\n",
    "#\n",
    "#iris = datasets.load_iris()\n",
    "#X = iris.data\n",
    "#y = iris.target\n",
    "#\n",
    "#X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4)\n",
    "#\n",
    "#nn = NeuralNet(reg_lambda = 2.1)\n",
    "#nn.fit(X_train, y_train)\n",
    "#\n",
    "#\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#\n",
    "#accuracy_score(y_test, nn.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47764338,  0.78715049,  0.46901537,  0.64170794],\n",
       "       [ 0.15625387,  0.93352675,  0.73788127,  0.78484598],\n",
       "       [ 0.41162111,  0.98616717,  0.80951491,  0.08323786]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3,4)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
