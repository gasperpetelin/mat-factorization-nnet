{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def readDataAndLabels(fileName):\n",
    "    train = genfromtxt(fileName, delimiter=',', dtype=float, skip_header=1)\n",
    "    return (train[:, 0], train[:, 1:])\n",
    "\n",
    "def oneHot(array):\n",
    "    m = np.max(array)\n",
    "    ar = np.zeros((len(array), m+1))\n",
    "    y=0\n",
    "    for x in array:\n",
    "        ar[y, x] = 1\n",
    "        y+=1\n",
    "    return ar\n",
    "\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, layerSizes, activationFunction):\n",
    "        self._layerSizes = layerSizes\n",
    "        self._activationFunction = activationFunction\n",
    "        self.initRandomWeights()\n",
    "        \n",
    "    def initRandomWeights(self):\n",
    "        self._weights = []\n",
    "        for layer in range(len(self._layerSizes)-1):\n",
    "            self._weights.append(np.random.randn(self._layerSizes[layer], self._layerSizes[layer+1]))\n",
    "        \n",
    "        self._biases = []\n",
    "        for bias in range(len(self._layerSizes)-1):\n",
    "            self._biases.append(np.random.randn(1, self._layerSizes[bias+1]))\n",
    "            \n",
    "    def getWeights(self):\n",
    "        return (self._weights, self._biases)\n",
    "    \n",
    "    #def getWeightsWithBiases(self):\n",
    "    #    weights = list()\n",
    "    #    for w,b in zip(self._weights, self._biases):\n",
    "    #        weights.append(np.concatenate((b, w), axis=0))\n",
    "    #    return weights\n",
    "    \n",
    "    def setWeights(self, weights, biases):\n",
    "        self._weights = weights\n",
    "        self._biases = biases\n",
    "    \n",
    "    #def forward(self, X):\n",
    "    #    zlist, aList = [],[X]\n",
    "    #    for weight, biases in zip(self._weights, self._biases):\n",
    "    #        X = X.dot(weight) + biases\n",
    "    #        zlist.append(X)\n",
    "    #        X = self._activationFunction.value(X)\n",
    "    #        aList.append(X)\n",
    "    #    return (X, zlist, aList)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for weight, biases in zip(self._weights, self._biases):\n",
    "            X = X.dot(weight) + biases\n",
    "            X = self._activationFunction.value(X)\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        for weight, biases in zip(self._weights, self._biases):\n",
    "            X = self._activationFunction.value(X.dot(weight) + biases)\n",
    "        return X \n",
    "    \n",
    "    def addOnesForBiasCalculation(self, X):\n",
    "        z = np.ones((len(X),1))\n",
    "        return np.concatenate((z, X), axis=1)\n",
    "    \n",
    "    def forwardTest(self, X):\n",
    "        self.z2 = np.dot(X, self._weights[0])\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self._weights[0])\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "\n",
    "    \n",
    "    def backpropagation(self, X, Y, costFunction, learningRate = 0.0001, callBackFunction = None):\n",
    "        \n",
    "        \n",
    "        self.yHat = self.forwardTest(X)\n",
    "        \n",
    "        callBackFunction(self.yHat, Y)\n",
    "        \n",
    "        delta3 = np.multiply(-(Y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        self._weights[0] -= dJdW1\n",
    "        self._weights[1] -= dJdW2\n",
    "        \n",
    "        #return dJdW1, dJdW2\n",
    "        \n",
    "        \n",
    "        #pred, zList, aList = self.forward(X)\n",
    "        #\n",
    "        #if callBackFunction is not None:\n",
    "        #    predY, _, _ = self.forward(X)\n",
    "        #    callBackFunction(predY, Y)\n",
    "        #\n",
    "        #error = costFunction.cost(pred, Y)\n",
    "        #print(error)\n",
    "        #zList = zList[::-1]\n",
    "        #aList = aList[::-1]\n",
    "\n",
    "        #delta3 = costFunction.delta(pred, Y, zList[0], self._activationFunction)\n",
    "        #dJdW2 = np.dot(aList[1].T, delta3)\n",
    "        #\n",
    "        #delta2 = np.dot(delta3, self._weights[1].T)*self._activationFunction.derivative(zList[1])\n",
    "        #dJdW1 = np.dot(X.T, delta2)\n",
    "        #\n",
    "        ##print(X.T.shape, aList[1].T.shape)\n",
    "        #\n",
    "        #self._weights[1] -= learningRate * dJdW2\n",
    "        #self._weights[0] -= learningRate * dJdW1\n",
    "        #\n",
    "        #self._biases[1] -= learningRate * np.sum(delta3, axis=0, keepdims=True)\n",
    "        #self._biases[0] -= learningRate * np.sum(delta2, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SigmoidActivationFunction:\n",
    "    def value(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def derivative(self, x):\n",
    "        v = self.value(x)\n",
    "        return np.multiply(v, 1-v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoidna funkcija in njen odvod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sigX =  np.arange(-10, 10, 0.1)\n",
    "#sigF = SigmoidActivationFunction()\n",
    "#plt.plot(sigX, sigF.value(sigX), linewidth=1.4)\n",
    "#plt.plot(sigX, sigF.derivative(sigX), linewidth=1.4)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align}\n",
    "J\n",
    "&= \\left[ \\frac{1}{m} \\sum_{i=1}^m \\left( \\frac{1}{2} \\left\\| h_{W,b}(x^{(i)}) - y^{(i)} \\right\\|^2 \\right) \\right]\n",
    "                       + \\frac{\\lambda}{2} \\sum_{l=1}^{n_l-1} \\; \\sum_{i=1}^{s_l} \\; \\sum_{j=1}^{s_{l+1}} \\left( W^{(l)}_{ji} \\right)^2\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MSECost:\n",
    "    def cost(self, predicted, actual):\n",
    "        return 0.5 * np.sum(np.power(predicted - actual, 2))/len(predicted)\n",
    "    \n",
    "    def delta(self, predicted, actual, z, activationFunction):\n",
    "        return np.multiply(-(actual-predicted), activationFunction.derivative(z))\n",
    "    \n",
    "class PredictionAccuracy:\n",
    "    @staticmethod\n",
    "    def CA(predicted, actual): #OneHot encoded data\n",
    "        neki = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datalabels, datapixels = readDataAndLabels('Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (42000,100) and (784,100) not aligned: 100 (dim 1) != 784 (dim 0)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7c2e2476eda2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mneuralNetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatapixels\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMSECost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallBackFunction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-1fa2e295fca8>\u001b[0m in \u001b[0;36mbackpropagation\u001b[0;34m(self, X, Y, costFunction, learningRate, callBackFunction)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myHat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforwardTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mcallBackFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myHat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1fa2e295fca8>\u001b[0m in \u001b[0;36mforwardTest\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0myHat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0myHat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (42000,100) and (784,100) not aligned: 100 (dim 1) != 784 (dim 0)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def callBackFunction(predicted, actual):\n",
    "    n = np.argmax(predicted, axis=1) - np.argmax(actual, axis=1)\n",
    "    #print(predicted[1,:], actual[1,:], n[1])\n",
    "    print(\"CA\", (len(n) - np.count_nonzero(n))/len(n))\n",
    "\n",
    "neuralNetwork = NeuralNet([784, 100, 10], SigmoidActivationFunction())\n",
    "#pr, zList, aList = neuralNetwork.forward(datapixels/255)\n",
    "actual = oneHot(datalabels.astype(int))\n",
    "    \n",
    "\n",
    "for x in range(50):\n",
    "    neuralNetwork.backpropagation(datapixels/255, actual, MSECost(), 0.1, callBackFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}