{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "def readDataAndLabels(fileName):\n",
    "    train = genfromtxt(fileName, delimiter=',', dtype=float, skip_header=1)\n",
    "    return (train[:, 0], train[:, 1:])\n",
    "\n",
    "def oneHot(array):\n",
    "    m = np.max(array)\n",
    "    ar = np.zeros((len(array), m+1))\n",
    "    y=0\n",
    "    for x in array:\n",
    "        ar[y, x] = 1\n",
    "        y+=1\n",
    "    return ar\n",
    "\n",
    "class SigmoidActivationFunction:\n",
    "    def value(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def derivative(self, x):\n",
    "        v = self.value(x)\n",
    "        return np.multiply(v, 1-v)\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, layerSizes, activationFunction):\n",
    "        self._layerSizes = layerSizes\n",
    "        self._activationFunction = activationFunction\n",
    "        self.initRandomWeights()\n",
    "        \n",
    "    def initRandomWeights(self):\n",
    "        self._weights = []\n",
    "        for layer in range(len(self._layerSizes)-1):\n",
    "            self._weights.append(np.random.randn(self._layerSizes[layer], self._layerSizes[layer+1]))\n",
    "        \n",
    "        self._biases = []\n",
    "        for bias in range(len(self._layerSizes)-1):\n",
    "            self._biases.append(np.random.randn(1, self._layerSizes[bias+1]))\n",
    "            \n",
    "    def getWeights(self):\n",
    "        return (self._weights, self._biases)\n",
    "    \n",
    "    def getWeightsWithBiases(self):\n",
    "        weights = list()\n",
    "        for w,b in zip(self._weights, self._biases):\n",
    "            weights.append(np.concatenate((b, w), axis=0))\n",
    "        return weights\n",
    "    \n",
    "    def setWeights(self, weights, biases):\n",
    "        self._weights = weights\n",
    "        self._biases = biases\n",
    "    \n",
    "    def forward(self, X):\n",
    "        zlist, aList = [],[]\n",
    "        for weight, biases in zip(self._weights, self._biases):\n",
    "            X = X.dot(weight) + biases\n",
    "            zlist.append(X)\n",
    "            X = self._activationFunction.value(X)\n",
    "            aList.append(X)\n",
    "        return (X, zlist, aList)\n",
    "    \n",
    "    def addOnesForBiasCalculation(self, X):\n",
    "        z = np.ones((len(X),1))\n",
    "        return np.concatenate((z, X), axis=1)\n",
    "\n",
    "    \n",
    "    def backpropagation(self, X, Y, costFunction, learningRate = 0.0001):\n",
    "        pred, zList, aList = self.forward(X)\n",
    "        error = costFunction.cost(pred, Y)\n",
    "        print(error)\n",
    "        zList = zList[::-1]\n",
    "        aList = aList[::-1]\n",
    "\n",
    "        delta3 = np.multiply(-(Y-pred), self._activationFunction.derivative(zList[0]))\n",
    "        dJdW2 = np.dot(aList[1].T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self._weights[1].T)*self._activationFunction.derivative(zList[1])\n",
    "        dJdW1 = np.dot(X.T, delta2)\n",
    "        \n",
    "        self._weights[1] -= learningRate * dJdW2\n",
    "        self._weights[0] -= learningRate * dJdW1\n",
    "        \n",
    "        \n",
    "        print(self._biases[1].shape, delta3.shape)\n",
    "        print(self._biases[0].shape, delta2.shape)\n",
    "        #self._biases[1] -= learningRate * np.mean(delta2, 0)\n",
    "        #self._biases[0] -= learningRate * np.mean(delta1, 0)\n",
    "        #print(self._weights[1].shape, dJdW2.shape)\n",
    "        #print(self._weights[0].shape, dJdW1.shape)\n",
    "        #return dJdW1, dJdW2\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align}\n",
    "J\n",
    "&= \\left[ \\frac{1}{m} \\sum_{i=1}^m \\left( \\frac{1}{2} \\left\\| h_{W,b}(x^{(i)}) - y^{(i)} \\right\\|^2 \\right) \\right]\n",
    "                       + \\frac{\\lambda}{2} \\sum_{l=1}^{n_l-1} \\; \\sum_{i=1}^{s_l} \\; \\sum_{j=1}^{s_{l+1}} \\left( W^{(l)}_{ji} \\right)^2\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSECost:\n",
    "    def cost(self, predicted, actual):\n",
    "        return 0.5 * np.sum(np.power(predicted - actual, 2))/len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datalabels, datapixels = readDataAndLabels('Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 25)\n",
      "(25, 10)\n",
      "(1, 25)\n",
      "(1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.75016627,  0.87476407],\n",
       "       [ 1.        ,  0.73669149,  0.64655557],\n",
       "       [ 1.        ,  0.73464359,  0.12799447]])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuralNetwork = NeuralNet([784, 25, 10], SigmoidActivationFunction())\n",
    "pr, zList, aList = neuralNetwork.forward(datapixels/255)\n",
    "actual = oneHot(datalabels.astype(int))\n",
    "\n",
    "weights, biases = neuralNetwork.getWeights()\n",
    "\n",
    "for x in weights:\n",
    "    print(x.shape)\n",
    "\n",
    "for x in biases:\n",
    "    print(x.shape)\n",
    "    \n",
    "\n",
    "#for x in range(30):\n",
    "#    neuralNetwork.backpropagation(datapixels/255, actual, MSECost(), 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255.0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#class Network(object):\n",
    "#\n",
    "#    def __init__(self, sizes):\n",
    "#        self.num_layers = len(sizes)\n",
    "#        self.sizes = sizes\n",
    "#        self.large_weight_initializer()\n",
    "#\n",
    "#    def large_weight_initializer(self):\n",
    "#        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "#        self.weights = [np.random.randn(y, x)\n",
    "#                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "#    def feedforward(self, a):\n",
    "#        for b, w in zip(self.biases, self.weights):\n",
    "#            a = np.dot(w, a)\n",
    "#            print(a.shape)\n",
    "#        return a\n",
    "#        \n",
    "#n = Network([784, 25, 10])\n",
    "#n.feedforward(datapixels.transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#oneHot(np.array([0,4,3,4,2,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.random.rand(3,2)\n",
    "B = np.random.rand(3,2)\n",
    "\n",
    "\n",
    "b = np.random.rand(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNet' object has no attribute 'addOnesForBiasCalculation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-330-0ffdfedb6704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneuralNetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddOnesForBiasCalculation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NeuralNet' object has no attribute 'addOnesForBiasCalculation'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97621369,  0.6617447 ],\n",
       "       [ 0.90703541,  0.88849807],\n",
       "       [ 0.20557865,  0.38961472]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39071981001258638"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((A-B)**2)/len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39071981001258638"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum((A-B)**2, 1))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
